{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_borders(dirty):\n",
    "    image = dirty.copy()\n",
    "    image[0] = np.zeros(len(image[0]))\n",
    "    image[-1] = np.zeros(len(image[0]))\n",
    "    image[:,0] = np.zeros(len(image[:,0]))\n",
    "    image[:,-1] = np.zeros(len(image[:,0]))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter1x1(dirty):\n",
    "    image = dirty.copy()\n",
    "    for v in np.arange(1,29):\n",
    "        for h in np.arange(1,139):\n",
    "            PXL = 0\n",
    "            PXL += (sum(image[v-1,h-1:h+2]) + sum(image[v+1,h-1:h+2]) +             sum(image[v-1:v+2,h-1]) + sum(image[v-1:v+2,h+1] ))\n",
    "            if PXL < 255 and image[v,h] != 0:\n",
    "                image[v,h] = 0\n",
    "    return image\n",
    "\n",
    "def filter2x2(dirty):\n",
    "    image = dirty.copy()    \n",
    "    for v in np.arange(1,28):\n",
    "        for h in np.arange(1,138):\n",
    "            PXL = 0\n",
    "            PXL += (sum(image[v-1,h-1:h+3]) + sum(image[v+2,h-1:h+3]) +             sum(image[v-1:v+3,h-1]) + sum(image[v-1:v+3,h+2]) )\n",
    "            if PXL < 255 and np.sum(image[v:v+2,h:h+2]) != 0:\n",
    "                image[v:v+2,h:h+2] = np.zeros(4).reshape((2,2))\n",
    "\n",
    "    return image\n",
    "\n",
    "def filter3x3(dirty):\n",
    "    image = dirty.copy()\n",
    "    for v in np.arange(1,27):\n",
    "        for h in np.arange(1,137):\n",
    "            PXL = 0\n",
    "            PXL += (sum(image[v-1,h-1:h+4]) + sum(image[v+3,h-1:h+4]) +             sum(image[v-1:v+4,h-1]) + sum(image[v-1:v+4, h+3]))\n",
    "            if PXL < 360 and np.sum(image[v:v+3,h:h+3]) > 255 and np.sum(image[v:v+3,h:h+3]) < 1720:\n",
    "                image[v:v+3,h:h+3] = np.zeros(9).reshape((3,3))\n",
    "    return image\n",
    "\n",
    "def filter3x1(dirty):\n",
    "    image = dirty.copy()    \n",
    "    for v in np.arange(1,27):\n",
    "        for h in np.arange(1,139):\n",
    "            PXL = 0\n",
    "            PXL += (sum(image[v-1:v+4,h-1]) + sum(image[v-1:v+4,h+1]) +            sum(image[v+3,h-1:h+2]) + sum(image[v-1,h-1:h+2]) )\n",
    "            if PXL < 255 and np.sum(image[v:v+3,h]) != 0:\n",
    "                image[v:v+3,h] = np.zeros(3).reshape((1,3))\n",
    "    return image\n",
    "\n",
    "def filter1x3(dirty):\n",
    "    image = dirty.copy()\n",
    "    for v in np.arange(1,29):\n",
    "        for h in np.arange(1,137):\n",
    "            PXL = 0\n",
    "            PXL += (sum(image[v-1,h-1:h+4]) + sum(image[v+1,h-1:h+4]) +             sum(image[v-1:v+2,h-1]) + sum(image[v-1:v+2,h+3]))\n",
    "            if PXL < 255 and np.sum(image[v,h:h+3]) != 0:\n",
    "                image[v,h:h+3] = np.zeros(3).reshape((1,3))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_clean(dirty):\n",
    "    image = dirty.copy()\n",
    "    PXL = 100\n",
    "    for v in np.arange(1,29):\n",
    "        for h in np.arange(1,139):\n",
    "            if sum(image[v-1,h-1:h+2]) > PXL and sum(image[v+1,h-1:h+2]) > PXL and             sum(image[v-1:v+2,h-1]) > PXL and sum(image[v-1:v+2,h+1]) > PXL and             image[v,h] < 30 and np.sum(image[v-1:v+2,h-1:h+2]) > 1000:\n",
    "                image[v,h] = (np.sum(image[v-1:v+2,h-1:h+2]) + 200) / 9\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_image_noise(image):\n",
    "    clean = clean_borders(image)\n",
    "    return negative_clean(filter1x1(filter1x3(filter3x1(filter2x2(filter3x3(clean))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all letter list functions \n",
    "def letter_list_3(image):\n",
    "    letter_list = []\n",
    "    identifier = []\n",
    "    i = 0        \n",
    "    while i < (np.shape(image)[1] -1):\n",
    "        if np.sum(image[:,i:i+2]) > 640:\n",
    "            identifier.append(i)\n",
    "    # Until now the separator works the same as in the parent function. The following part is different.\n",
    "    # The frame that is being recorded is bigger (48x48) if possible, so that combined letters can be separated later on.\n",
    "        elif sum(identifier) != 0:\n",
    "            CTR = np.int(np.mean(identifier))\n",
    "            if CTR < 12:\n",
    "                x = image[1:-1, 0: 28]\n",
    "            elif CTR < 20:\n",
    "                x = image[1:-1, CTR-12: CTR+16]\n",
    "            elif CTR > 112:\n",
    "                CTR = 112\n",
    "                x = image[1:-1, CTR-20: CTR+28]\n",
    "            else:\n",
    "                x = image[1:-1, CTR-20: CTR+28]\n",
    "            letter_list.append(x)\n",
    "            identifier = []\n",
    "            i+= 5\n",
    "        i += 1\n",
    "    density = []\n",
    "    \n",
    "    # The identified letters are being transformed into a single value, stored in the density list.\n",
    "    # Value is based on the sum of all pixels in the frame.\n",
    "    # The letter with the highest density is split into two 28x28 frames, this is often the double letter.\n",
    "    for x in letter_list:\n",
    "        density = np.append(density, np.sum(x))\n",
    "        double_letter = letter_list[np.argmax(density)]\n",
    "        if np.shape(double_letter) == (28,28):\n",
    "            try:\n",
    "                double_letter = letter_list[np.argmax(density) + 1]\n",
    "            except:\n",
    "                double_letter = letter_list[np.argmax(density) - 1]\n",
    "    single_letter1 = double_letter[:, 0:28]\n",
    "    single_letter2 = double_letter[:,20:] \n",
    "\n",
    "    # remaining 48x48 frames are being transformed into 28x28 frames.    \n",
    "    for i, x in enumerate(letter_list):\n",
    "        if np.shape(x) != (28,28):\n",
    "            letter_list[i] = x[:,10:38]\n",
    "\n",
    "    # The separated letters are placed back on the correct position and the old combined frame is removed.\n",
    "    good_list = []\n",
    "    for i, x in enumerate(letter_list):\n",
    "        if i == np.argmax(density):\n",
    "            good_list.append(single_letter1)\n",
    "            good_list.append(single_letter2)\n",
    "        else:\n",
    "            good_list.append(x)\n",
    "        \n",
    "    return good_list\n",
    "        \n",
    "def letter_list_2(image):\n",
    "    letter_list = []\n",
    "    identifier = []\n",
    "    i = 0\n",
    "        \n",
    "    while i < (np.shape(image)[1] -1):\n",
    "        if np.sum(image[:,i:i+2]) > 640:\n",
    "            identifier.append(i)\n",
    "            \n",
    "    # Until now the separator works the same as in the parent function. The following part is different.\n",
    "    # The frame that is being recorded is bigger (48x48) if possible, so that combined letters can be separated later on.\n",
    "        elif sum(identifier) != 0:\n",
    "            CTR = np.int(np.mean(identifier))\n",
    "            if CTR < 20:\n",
    "                CTR = 20\n",
    "            if CTR > 112:\n",
    "                CTR = 112\n",
    "            x = image[1:-1, CTR-20: CTR+28]\n",
    "            letter_list.append(x)\n",
    "            identifier = []\n",
    "            i+= 5\n",
    "        i += 1\n",
    "\n",
    "    new_list = []\n",
    "    for x in letter_list:\n",
    "        new_list.append(x[:, 0:28])\n",
    "        new_list.append(x[:,20:])\n",
    "    return new_list\n",
    "\n",
    "def letter_list_1(image):\n",
    "    letter_list = []\n",
    "    identifier = []\n",
    "    i = 0\n",
    "    \n",
    "    # While-loop that detects start and beginning of individual letter in a 'Captcha'.\n",
    "    # A 28x28 frame around the detected center of the letter is stored in the letter_list.\n",
    "    while i < (np.shape(image)[1] -1):\n",
    "        if np.sum(image[:,i:i+2]) > 2295:\n",
    "            identifier.append(i)\n",
    "        elif sum(identifier) != 0:\n",
    "            CTR = np.int(np.mean(identifier))\n",
    "            if CTR < 12:\n",
    "                CTR = 12\n",
    "            elif CTR > 124:\n",
    "                CTR = 124\n",
    "            x = image[1:-1, CTR-12: CTR+16]\n",
    "            letter_list.append(x)\n",
    "            identifier = []\n",
    "            i+= 5        \n",
    "        i += 1    \n",
    "    return letter_list\n",
    "\n",
    "def letter_list_plus(letter_list):\n",
    "    for x in np.arange(len(letter_list) - 4):\n",
    "        density = []\n",
    "        for x in letter_list:\n",
    "            density = np.append(density, np.sum(x))\n",
    "\n",
    "        for i, x in enumerate(letter_list):\n",
    "            if i == np.argmin(density):\n",
    "                continue\n",
    "            letter_list[i] = x\n",
    "    return letter_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate the image vertically using the letter_list functions\n",
    "def vertical_separator(image):\n",
    "    letter_list = []\n",
    "    identifier = []\n",
    "    i = 0    \n",
    "    # While-loop that detects start and beginning of individual letter in a 'Captcha'.\n",
    "    # A 28x28 frame around the detected center of the letter is stored in the letter_list.\n",
    "    while i < (np.shape(image)[1] -1):\n",
    "        if np.sum(image[:,i:i+2]) > 640:\n",
    "            identifier.append(i)        \n",
    "        elif sum(identifier) != 0:\n",
    "            CTR = np.int(np.mean(identifier))\n",
    "            if CTR < 12:\n",
    "                CTR = 12\n",
    "            elif CTR > 124:\n",
    "                CTR = 124\n",
    "            x = image[1:-1, CTR-12: CTR+16]\n",
    "            letter_list.append(x)\n",
    "            identifier = []\n",
    "            i+= 5        \n",
    "        i += 1            \n",
    "    # While the above function is prone to error, the following if statements check whether the desired results are present. Otherwise different separator is used.  \n",
    "    if len(letter_list) == 4:\n",
    "        return letter_list\n",
    "    \n",
    "    if len(letter_list) == 3:\n",
    "        return letter_list_3(image)\n",
    "    \n",
    "    if len(letter_list) == 2:\n",
    "        return letter_list_2(image)\n",
    "\n",
    "    if len(letter_list) == 1:\n",
    "        return letter_list_1(image)\n",
    "    \n",
    "    if len(letter_list) > 4:\n",
    "        return letter_list_plus(letter_list)    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation sets and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createValidationSets(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.9, random_state=999)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.7777778, random_state=999)\n",
    "    return x_train, x_test, x_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotFitTransform(y_train, y_val, y_test):\n",
    "    onehot = LabelBinarizer()\n",
    "    y_train = onehot.fit_transform(y_train)\n",
    "    y_val   = onehot.transform(y_val)\n",
    "    y_test   = onehot.transform(y_test)\n",
    "    return y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only used for the accuracy score \n",
    "def oneHotInverseTransform(y_pred):\n",
    "    return onehot.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape \n",
    "def reshape_28(array):\n",
    "    new = []\n",
    "    for i, x in enumerate(array):\n",
    "        \n",
    "        new.append(np.reshape(x,(28,28)))\n",
    "    return np.array(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop image\n",
    "def cropped_image(image, filter_size):\n",
    "    image_size = np.shape(image)[0]\n",
    "    cropped_image = []\n",
    "    # Vertical movement of the filter\n",
    "    for v in np.arange(0,image_size, filter_size):\n",
    "        # Horizontal movement of the filter\n",
    "        for x in np.arange(0,image_size, filter_size):\n",
    "            cropped_image.append(np.mean(image[v:v+filter_size,x: x + filter_size]))\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get image width old\n",
    "# def width(image):\n",
    "#     width_list = []\n",
    "#     for i, x in enumerate(np.arange(0,np.shape(image)[0] -1)):\n",
    "#         if sum(image[:,x]) > 300:\n",
    "#             width_list.append(i)\n",
    "#             break\n",
    "#     for i, x in enumerate(np.arange(np.shape(image)[0] -1, 0, -1)):\n",
    "#         if sum(image[:,x]) > 300:\n",
    "#             width_list.append(np.shape(image)[0] - i)\n",
    "#             break\n",
    "#     return width_list[1] - width_list[0] \n",
    "\n",
    "#new\n",
    "def width(image):\n",
    "    width_list = []\n",
    "    for i, x in enumerate(np.arange(0,np.shape(image)[0] -1)):\n",
    "        if sum(image[:,x]) > 300:\n",
    "            width_list.append(i)\n",
    "            break\n",
    "    for i, x in enumerate(np.arange(np.shape(image)[0] -1, 0, -1)):\n",
    "        if sum(image[:,x]) > 300:\n",
    "            width_list.append(np.shape(image)[0] - i)\n",
    "            break\n",
    "    if len(width_list) != 2:\n",
    "        return 20\n",
    "    return width_list[1] - width_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get image heigth old\n",
    "# def heigth(image):\n",
    "#     heigth_list = []\n",
    "#     for i, x in enumerate(np.arange(0,np.shape(image)[0] -1)):\n",
    "#         if sum(image[x,:]) > 300:\n",
    "#             heigth_list.append(i)\n",
    "#             break\n",
    "#     for i, x in enumerate(np.arange(np.shape(image)[0] -1, 0, -1)):\n",
    "#         if sum(image[x,:]) > 300:\n",
    "#             heigth_list.append(np.shape(image)[0] - i)\n",
    "#             break\n",
    "#     return heigth_list[1] - heigth_list[0]   \n",
    "\n",
    "#new\n",
    "def heigth(image):\n",
    "    heigth_list = []\n",
    "    for i, x in enumerate(np.arange(0,np.shape(image)[0] -1)):\n",
    "        if sum(image[x,:]) > 300:\n",
    "            heigth_list.append(i)\n",
    "            break\n",
    "    for i, x in enumerate(np.arange(np.shape(image)[0] -1, 0, -1)):\n",
    "        if sum(image[x,:]) > 300:\n",
    "            heigth_list.append(np.shape(image)[0] - i)\n",
    "            break\n",
    "    if len(heigth_list) != 2:\n",
    "        return 20\n",
    "    return heigth_list[1] - heigth_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get density\n",
    "def density(image):\n",
    "    return  np.sum(image) / (255 * 28 *28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get edge count\n",
    "def edge_count(image):\n",
    "    flat = image.flatten()\n",
    "    safety_margin = int(0.05*255)\n",
    "    edge_count = 0\n",
    "    for x in flat:\n",
    "        if x > 0 + safety_margin and x < 255 - safety_margin:\n",
    "            edge_count += 1\n",
    "    return edge_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitTransformUsingScaler(data):\n",
    "    scaler = MaxAbsScaler()\n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale for CNN model\n",
    "def scale(X):\n",
    "    scale = X.astype(dtype = 'float64')\n",
    "    for i, x in enumerate(scale):\n",
    "        scale[i] = x / 255.\n",
    "    return scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createKerasModel():\n",
    "    # Hyperparameters:\n",
    "    hidden_nodes = 1024 #?\n",
    "    output_size = 26\n",
    "    layer = 'selu'\n",
    "    kernel_init = 'lecun_normal'\n",
    "    NUM_EPOCHS = 100 \n",
    "    eta = 0.00005\n",
    "    regularizer = tf.keras.regularizers.l2(0.01)\n",
    "    nadam = tf.keras.optimizers.Nadam(learning_rate=eta, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\")\n",
    "    adam = tf.keras.optimizers.Adam(\n",
    "        learning_rate= eta,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name=\"Adam\",\n",
    "    )\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(1024, input_dim=269, activation=layer, kernel_initializer = kernel_init, use_bias = False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(512, activation=layer, kernel_initializer = kernel_init, use_bias = False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(512, activation=layer, kernel_initializer = kernel_init, use_bias = False), \n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(512, activation=layer, kernel_initializer = kernel_init, use_bias = False),\n",
    "        tf.keras.layers.Dense(output_size, activation = 'softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN_model():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", kernel_initializer='lecun_normal'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", kernel_initializer='lecun_normal'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(26, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    optimizer = tensorflow.keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name=\"Adam\",)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the top scores from the predictions and replace the lowest score \n",
    "#from all letters with the next best performing predictions\n",
    "def getTopScores(all_letter_scores, all_letters):\n",
    "    secondUsedIndex = []\n",
    "    thirdUsedIndex = []\n",
    "    fourthUsedIndex = []\n",
    "    \n",
    "    resultList = []\n",
    "    firstScores = []\n",
    "    secondScores = []\n",
    "    thirdScores = []\n",
    "    fourthScores = []\n",
    "    fifthScores = []\n",
    "    \n",
    "    firstPredictions = []\n",
    "    secondPredictions = []\n",
    "    thirdPredictions = []\n",
    "    fourthPredictions = []\n",
    "    fifthPredictions = []\n",
    "    \n",
    "    index = 0\n",
    "    #add all predictions to a list based on their score\n",
    "    for scores in all_letter_scores: \n",
    "        firstScores.append(scores[0]) \n",
    "        secondScores.append(scores[1])\n",
    "        thirdScores.append(scores[2])\n",
    "        fourthScores.append(scores[3])\n",
    "        fifthScores.append(scores[4])\n",
    "        \n",
    "        firstPredictions.append(all_letters[index][0]) \n",
    "        secondPredictions.append(all_letters[index][1])\n",
    "        thirdPredictions.append(all_letters[index][2])\n",
    "        fourthPredictions.append(all_letters[index][3])\n",
    "        fifthPredictions.append(all_letters[index][4])\n",
    "        \n",
    "        index +=1    \n",
    "    scoreList = [[]]*5\n",
    "    \n",
    "    #update the score_list with the highest probability for the 4 letters\n",
    "    for i in range(5):       \n",
    "        completeStr = \"\"    \n",
    "        indexLowest = firstScores.index(min(firstScores))\n",
    "        for strScore in firstPredictions:\n",
    "            #for the sake of reference add to new object\n",
    "            completeStr += str(strScore).zfill(2)\n",
    "        scoreList[i] = completeStr   \n",
    "        \n",
    "        #if the index is not used before get value from second position otherwise from next\n",
    "        if (indexLowest not in secondUsedIndex):\n",
    "            secondUsedIndex.append(indexLowest)\n",
    "            firstScores[indexLowest] = secondScores[indexLowest]\n",
    "            firstPredictions[indexLowest] = secondPredictions[indexLowest]\n",
    "        elif (indexLowest not in thirdUsedIndex):\n",
    "            thirdUsedIndex.append(indexLowest)\n",
    "            firstScores[indexLowest] = thirdScores[indexLowest]\n",
    "            firstPredictions[indexLowest] = thirdPredictions[indexLowest]\n",
    "        elif (indexLowest not in fourthUsedIndex):\n",
    "            fourthUsedIndex.append(indexLowest)\n",
    "            firstScores[indexLowest] = fourthScores[indexLowest]\n",
    "            firstPredictions[indexLowest] = fourthPredictions[indexLowest]\n",
    "        else: \n",
    "            firstScores[indexLowest] = fifthScores[indexLowest]\n",
    "            firstPredictions[indexLowest] = fifthPredictions[indexLowest]\n",
    "    \n",
    "    return scoreList "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictImages(images, model):\n",
    "    output_list = []\n",
    "    output_list.append(model.predict(images))\n",
    "\n",
    "    all_letter_scores = []\n",
    "    all_letters = []\n",
    "    letterIndex = range(1, 27)\n",
    "    #loop over the predictions\n",
    "    for pred in output_list:\n",
    "        letterScoreList = []\n",
    "        letterList = []\n",
    "        \n",
    "        #get the top 5 predictions\n",
    "        score = sorted(zip(pred, letterIndex), reverse=True)[:5]\n",
    "        letterScoreList.append(score[0][0])\n",
    "        letterList.append(score[0][1])\n",
    "\n",
    "        letterScoreList.append(score[1][0])\n",
    "        letterList.append(score[1][1])\n",
    "\n",
    "        letterScoreList.append(score[2][0])\n",
    "        letterList.append(score[2][1])\n",
    "\n",
    "        letterScoreList.append(score[3][0])\n",
    "        letterList.append(score[3][1])\n",
    "\n",
    "        letterScoreList.append(score[4][0])\n",
    "        letterList.append(score[4][1])\n",
    "\n",
    "        all_letter_scores.append(letterScoreList)\n",
    "        all_letters.append(letterList)\n",
    "    #get the top 5 digit predictions\n",
    "    return getTopScores(all_letter_scores, all_letters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "letterIndex = range(0, 26)\n",
    "print(len(letterIndex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write prediction to file\n",
    "def toPredictionFile(all_scores):\n",
    "    with open('predictionCNN.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "import numpy as np\n",
    "trainData = np.load(\"training-dataset.npz\")\n",
    "testData = np.load(\"test-dataset.npy\")\n",
    "\n",
    "#loadData\n",
    "x = trainData['x']\n",
    "y = trainData['y']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #Scale data\n",
    "# x = fitTransformUsingScaler(x)\n",
    "\n",
    "# #Create validation sets\n",
    "#x_train, x_test, x_val, y_train, y_test, y_val = createValidationSets(x, y)\n",
    "\n",
    "# #OneHot fit and transform\n",
    "# y_train, y_val, _ = oneHotFitTransform(y_train, y_val, _)\n",
    "\n",
    "#Create keras NN model\n",
    "# model = createKerasModel()\n",
    "\n",
    "# #fit the model with on the training data\n",
    "# model.fit(x_train,y_train, epochs=100, validation_data = (x_val, y_val), verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #TODO remove\n",
    "# #for now use pre-trained model\n",
    "# model = c('DNN_93%.tf')\n",
    "\n",
    "\n",
    "# # #Clean the images\n",
    "# cleaned_images = []\n",
    "# for x in testData[0:2]:\n",
    "#     clean_image = remove_image_noise(x)\n",
    "#     cleaned_images.append(vertical_separator(clean_image))\n",
    "\n",
    "# # predict\n",
    "# all_scores = []\n",
    "# for images in cleaned_images:    \n",
    "#     all_scores.append(predictImages(images, model))\n",
    "\n",
    "# #export to file\n",
    "# toPredictionFile(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create CNN model\n",
    "model = train_CNN_model()\n",
    "#fit CNN model\n",
    "\n",
    "\n",
    "#reshape for CNN \n",
    "# x = scale(x)\n",
    "# x = x.reshape((124800,28,28))\n",
    "\n",
    "#Create validation sets again after reshaping\n",
    "# x_train, x_test, x_val, y_train, y_test, y_val = createValidationSets(x, y)\n",
    "\n",
    "\n",
    "#shape train sets for CNN model\n",
    "# x_train = np.expand_dims(x_train, -1 )\n",
    "# x_val = np.expand_dims(x_val, -1 )\n",
    "# x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# #OneHot fit and transform\n",
    "# y_train, y_val, y_test = oneHotFitTransform(y_train, y_val, y_test)\n",
    "\n",
    "\n",
    "\n",
    "#model.fit(x_train, y_train,  epochs=25, validation_data=(x_val, y_val), verbose=1)\n",
    "\n",
    "#todo for now use pre-trained model\n",
    "model = keras.models.load_model(\"cnn_model.h5\")\n",
    "\n",
    "# #Clean the images\n",
    "cleaned_images = []\n",
    "all_scores = []\n",
    "#TODO remove [0:2]\n",
    "for x in testData:\n",
    "    clean_image = remove_image_noise(x)\n",
    "    seperated = vertical_separator(clean_image)\n",
    "    for img in seperated:\n",
    "        cleaned_images.append(img)\n",
    "#     print(np.array(cleaned_images[0]).shape)\n",
    "#     images.append(cleaned_images[0])\n",
    "#     images = np.array(cleaned_images[0])\n",
    "#     images = np.expand_dims(images, -1 )\n",
    "    \n",
    "images = np.array(cleaned_images)   \n",
    "images = np.expand_dims(images, -1 )\n",
    "#all_scores.append(predictImages(images, model))\n",
    "\n",
    "\n",
    "\n",
    "#not using the function \"predictImages()\" because this model need a slightly different approach \n",
    "output_list = []\n",
    "output_list.append(model.predict(images))\n",
    "\n",
    "all_letter_scores = []\n",
    "all_letters = []\n",
    "letterIndex = range(1, 27)\n",
    "#loop over the predictions\n",
    "for pred in output_list[0]:\n",
    "    letterScoreList = []\n",
    "    letterList = []\n",
    "\n",
    "    #get the top 5 predictions\n",
    "    score = sorted(zip(pred, letterIndex), reverse=True)[:5]\n",
    "    letterScoreList.append(score[0][0])\n",
    "    letterList.append(score[0][1])\n",
    "\n",
    "    letterScoreList.append(score[1][0])\n",
    "    letterList.append(score[1][1])\n",
    "\n",
    "    letterScoreList.append(score[2][0])\n",
    "    letterList.append(score[2][1])\n",
    "\n",
    "    letterScoreList.append(score[3][0])\n",
    "    letterList.append(score[3][1])\n",
    "\n",
    "    letterScoreList.append(score[4][0])\n",
    "    letterList.append(score[4][1])\n",
    "\n",
    "    all_letter_scores.append(letterScoreList)\n",
    "    all_letters.append(letterList)\n",
    "\n",
    "\n",
    "\n",
    "resultList = []\n",
    "firstScores = []\n",
    "secondScores = []\n",
    "thirdScores = []\n",
    "fourthScores = []\n",
    "fifthScores = []\n",
    "\n",
    "firstPredictions = []\n",
    "secondPredictions = []\n",
    "thirdPredictions = []\n",
    "fourthPredictions = []\n",
    "fifthPredictions = []\n",
    "\n",
    "index = 0\n",
    "#add all predictions to a list based on their score\n",
    "for scores in all_letter_scores:\n",
    "    firstScores.append(scores[0]) \n",
    "    secondScores.append(scores[1])\n",
    "    thirdScores.append(scores[2])\n",
    "    fourthScores.append(scores[3])\n",
    "    fifthScores.append(scores[4])\n",
    "\n",
    "    firstPredictions.append(all_letters[index][0]) \n",
    "    secondPredictions.append(all_letters[index][1])\n",
    "    thirdPredictions.append(all_letters[index][2])\n",
    "    fourthPredictions.append(all_letters[index][3])\n",
    "    fifthPredictions.append(all_letters[index][4])\n",
    "\n",
    "    index +=1    \n",
    "\n",
    "\n",
    "#split into list of all scores per letter\n",
    "firstScores_list = [firstScores[x:x+4] for x in range(0, len(firstScores), 4)]\n",
    "secondScores_list = [secondScores[x:x+4] for x in range(0, len(secondScores), 4)]\n",
    "thirdScores_list = [thirdScores[x:x+4] for x in range(0, len(thirdScores), 4)]\n",
    "fourthScores_list = [fourthScores[x:x+4] for x in range(0, len(fourthScores), 4)]\n",
    "fifthScores_list = [fifthScores[x:x+4] for x in range(0, len(fifthScores), 4)]\n",
    "\n",
    "#split into list of all predictions per letter\n",
    "firstPredictions_list = [firstPredictions[x:x+4] for x in range(0, len(firstPredictions), 4)]\n",
    "secondPredictions_list = [secondPredictions[x:x+4] for x in range(0, len(secondPredictions), 4)]\n",
    "thirdPredictions_list = [thirdPredictions[x:x+4] for x in range(0, len(thirdPredictions), 4)]\n",
    "fourthPredictions_list = [fourthPredictions[x:x+4] for x in range(0, len(fourthPredictions), 4)]\n",
    "fifthPredictions_list = [fifthPredictions[x:x+4] for x in range(0, len(fifthPredictions), 4)]\n",
    "\n",
    "\n",
    "\n",
    "list_counter = 0\n",
    "score_list_collection = []\n",
    "#update the score_list with the highest probability for the 4 letters\n",
    "for fromFirstScores in firstScores_list:\n",
    "    secondUsedIndex = []\n",
    "    thirdUsedIndex = []\n",
    "    fourthUsedIndex = []\n",
    "    scoreList = [[]]*5\n",
    "    for i in range(5):       \n",
    "        completeStr = \"\"  \n",
    "       \n",
    "\n",
    "        indexLowest = fromFirstScores.index(min(fromFirstScores))\n",
    "        for strScore in firstPredictions_list[list_counter]:\n",
    "            #for the sake of reference add to new object\n",
    "            completeStr += str(strScore).zfill(2)\n",
    "\n",
    "        scoreList[i] = completeStr   \n",
    "\n",
    "        #if the index is not used before get value from second position otherwise from next\n",
    "        if (indexLowest not in secondUsedIndex):\n",
    "            secondUsedIndex.append(indexLowest)\n",
    "            fromFirstScores[indexLowest] = secondScores_list[list_counter][indexLowest]\n",
    "            firstPredictions_list[list_counter][indexLowest] = secondPredictions_list[list_counter][indexLowest]\n",
    "        elif (indexLowest not in thirdUsedIndex):\n",
    "            thirdUsedIndex.append(indexLowest)\n",
    "            fromFirstScores[indexLowest] = thirdScores_list[list_counter][indexLowest]\n",
    "            firstPredictions_list[list_counter][indexLowest] = thirdPredictions_list[list_counter][indexLowest]\n",
    "        elif (indexLowest not in fourthUsedIndex):\n",
    "            fourthUsedIndex.append(indexLowest)\n",
    "            fromFirstScores[indexLowest] = fourthScores_list[list_counter][indexLowest]\n",
    "            firstPredictions_list[list_counter][indexLowest] = fourthPredictions_list[list_counter][indexLowest]\n",
    "        else: \n",
    "            fromFirstScores[indexLowest] = fifthScores[indexLowest]\n",
    "            firstPredictions[indexLowest] = fifthPredictions[indexLowest]\n",
    "    score_list_collection.append(scoreList)\n",
    "    list_counter +=1\n",
    "    \n",
    "all_scores = score_list_collection\n",
    "\n",
    "\n",
    "# #export to file\n",
    "toPredictionFile(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 14:53:52\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "#please note this started at 22:40\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
